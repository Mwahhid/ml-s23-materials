{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfb2a24-05ab-4b1d-8fc2-7448458663bd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=goodplace2.png align=right width=500>\n",
    "\n",
    "The TV show *The Good Place* is centered around a number of humans who have died and find themselves in the afterlife.  In this conception\n",
    "of the afterlife, humans are sent to \"the Good Place\" or \"the Bad Place\" after death.  All humans are assigned a numerical score based on the morality of their conduct in life, and only those with the very highest scores are sent to the \"Good Place\", where they enjoy eternal happiness; all others experience an eternity of torture in the \"Bad Place.\"\n",
    "\n",
    "In this project, you will explore using logistic regression to predict whether someone will end up in the \"Good Place or the \"Bad Place\" based on an\n",
    "extremely scaled down version of their conduct in life.  In particular, we have data for 1000 people about how often they:\n",
    "\n",
    "- Let someone merge in front of them in traffic\n",
    "- Didn't tip their server at a restaurant\n",
    "- Held a door open for someone who was walking behind them\n",
    "- Littered\n",
    "\n",
    "These will be our four features for the problem.  Our data set consists of these four features tallied for 1000 different people.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a37aed2-9e9c-43c6-a7fb-0d93fddff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10cb214-0b0d-4fbc-b6e9-38a4b4e066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"data1.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "df = pd.read_csv(\"data1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ce96-70c8-4ae6-aecf-2d06692fe2e8",
   "metadata": {},
   "source": [
    "## Explanation of the data file\n",
    "\n",
    "Each row of the file represents data about a person.  \n",
    "\n",
    "The first four columns should be self-explanatory: they tell how often a person did a\n",
    "certain activity (explained above).  There are two columns at the end saying whether they\n",
    "ended up in the \"Good Place\" or the \"Bad Place.\"\n",
    "\n",
    "The first of the two (`goodbad`) is calculated \"perfectly\" from a formula I came up with (that I'm keeping secret!)\n",
    "\"Perfectly\" meaning that the formula itself probably isn't perfect, but the good/bad column is calculated\n",
    "directly mathematically from the formula, based on the four features.\n",
    "\n",
    "The second of the two (`noisygoodbad`) is also calculated perfectly from the formula, but\n",
    "with some \"noise\" thrown in.  In other words, I've switched a few of the goods to bads and vice versa, to\n",
    "simulate a real-world situation (where the Good Place/Bad Place determination is based not only on this\n",
    "data, but other data as well that we don't have access to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3786e4-a047-4030-bf20-e97b083b15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few and last few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 1000\n",
    "df  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88a4df79-02ae-47de-aef3-c381bd00a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our X and y data\n",
    "\n",
    "# First, we will split the data frame above into a four-column frame\n",
    "# with the input features (X's) and a one-column frame with the target\n",
    "# feature (y), which we will use the noisy column (noisygoodbad).\n",
    "\n",
    "# Write code below to create df_X with just the four X feature columns,\n",
    "# and df_y that has just the noisygoodbad column.\n",
    "\n",
    "# Then **normalize** the X values with Z-score normalization as in \n",
    "# project 1.\n",
    "\n",
    "df_X = df[['letMerge', 'noTip', 'heldDoor', 'littered']]\n",
    "df_y = df['noisygoodbad']\n",
    "\n",
    "dfxMean = df_X.mean()\n",
    "dfxStd = df_X.std()\n",
    "\n",
    "for j in range(0, 4):\n",
    "    for i in range(0, 1000):\n",
    "        df_X.iat[i, j] = (df_X.iat[i,j] - dfxMean[j]) / dfxStd[j]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db28502-695c-4caa-994c-d5bd1f929bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.161413</td>\n",
       "      <td>-1.267048</td>\n",
       "      <td>-1.350553</td>\n",
       "      <td>-1.620752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.989370</td>\n",
       "      <td>2.629464</td>\n",
       "      <td>0.136883</td>\n",
       "      <td>1.396890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.707846</td>\n",
       "      <td>-0.487746</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>0.812830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.669471</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.601198</td>\n",
       "      <td>0.965798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.700751</td>\n",
       "      <td>1.070859</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.715487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.786047</td>\n",
       "      <td>-0.974810</td>\n",
       "      <td>-0.980621</td>\n",
       "      <td>0.187052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.130132</td>\n",
       "      <td>0.388969</td>\n",
       "      <td>1.246679</td>\n",
       "      <td>1.535952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.316840</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>1.562663</td>\n",
       "      <td>-0.146696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.473242</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.485594</td>\n",
       "      <td>-0.967161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.840538</td>\n",
       "      <td>-0.877397</td>\n",
       "      <td>1.308335</td>\n",
       "      <td>-0.689037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge     noTip  heldDoor  littered\n",
       "0   -1.161413 -1.267048 -1.350553 -1.620752\n",
       "1   -0.989370  2.629464  0.136883  1.396890\n",
       "2   -0.707846 -0.487746  0.607005  0.812830\n",
       "3    1.669471  0.681208  1.601198  0.965798\n",
       "4    1.700751  1.070859  0.761143  0.715487\n",
       "..        ...       ...       ...       ...\n",
       "995 -0.786047 -0.974810 -0.980621  0.187052\n",
       "996 -1.130132  0.388969  1.246679  1.535952\n",
       "997 -0.316840  0.876033  1.562663 -0.146696\n",
       "998 -0.473242  0.681208  1.485594 -0.967161\n",
       "999  0.840538 -0.877397  1.308335 -0.689037\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_X\n",
    "\n",
    "df_X  # Should print a data frame with 1000 rows and 4 columns.\n",
    "# First row should be [ -1.161413 -1.267048 -1.350553 -1.620752]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f31299c-bed6-44be-bd11-bb5eb84ba006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      good\n",
       "1       bad\n",
       "2      good\n",
       "3      good\n",
       "4      good\n",
       "       ... \n",
       "995     bad\n",
       "996     bad\n",
       "997    good\n",
       "998    good\n",
       "999    good\n",
       "Name: noisygoodbad, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_y\n",
    "\n",
    "df_y  # Should be a column of goods and bads, starting with good, bad, good, good, good...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6667ab3c-7106-4b57-b612-4756034136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    1\n",
       "998    1\n",
       "999    1\n",
       "Name: noisygoodbad, Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to switch df_y to not have good/bad strings, but rather 0's and 1's.\n",
    "# Use this line of code:\n",
    "\n",
    "df_y = (df_y == 'good').astype(int)\n",
    "\n",
    "# Sanity check: should now be a column of ones and zeros, with 1=good, 0=bad.  \n",
    "df_y   # Should begin 1, 0, 1, 1, 1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d6faa7-5308-4884-ac49-ed5af1eb15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check shapes:\n",
    "\n",
    "print(df_X.shape) # Should be (1000, 4)\n",
    "print(df_y.shape) # Should be (1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7ea4ac9-0c3c-470b-8425-2da67ca44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "\n",
    "# We want to write code to split the data frame above into a few\n",
    "# new data frames.  In particular, are going to have a TRAINING SET\n",
    "# and a TESTING SET for this project.  We will use 80% of the data for \n",
    "# training, and the remaining 20% for testing.  \n",
    "\n",
    "# In the real world, we would split the data randomly, but so we all\n",
    "# end up with the same results, we will use the first 80% of the data\n",
    "# for training, and the last 20% for testing (in order of how the rows\n",
    "# show up in the file).  Note that there are 1000 people (rows in \n",
    "# the file), so the first 800 rows will be training, and the last 200\n",
    "# will be testing.\n",
    "\n",
    "# Write code here to create FOUR NUMPY ndarrays:\n",
    "\n",
    "# - X_train: first 800 lines of df_X\n",
    "# - X_test: last 200 lines of df_X\n",
    "# - y_train: first 800 lines of df_y\n",
    "# - y_test: last 200 lines of df_y\n",
    "\n",
    "X_train = df_X[:800].to_numpy()\n",
    "X_test = df_X[800:].to_numpy()\n",
    "y_train = df_y[:800].to_numpy()\n",
    "y_test = df_y[800:].to_numpy()\n",
    "\n",
    "# Then, add a column of ones to the left side of X_train and X_test.\n",
    "\n",
    "X_train = np.hstack((np.ones((800, 1)), X_train))\n",
    "X_test = np.hstack((np.ones((200, 1)), X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a3f59aa-ba82-4ec5-9234-b390140e4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(800,)\n",
      "(200, 5)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks:\n",
    "\n",
    "print(X_train.shape) # Should be (800, 5)\n",
    "print(y_train.shape) # Should be (800,) \n",
    "print(X_test.shape) # Should be (200, 5)\n",
    "print(y_test.shape) # Should be (200,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "129456c3-f616-41bd-a93e-79d585030e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training examples:\n",
      "[[ 1.         -1.16141295 -1.26704849 -1.35055312 -1.62075162  1.        ]\n",
      " [ 1.         -0.98937031  2.62946412  0.13688262  1.3968899   0.        ]\n",
      " [ 1.         -0.70784599 -0.48774597  0.6070048   0.81283025  1.        ]\n",
      " [ 1.          1.66947051  0.68120782  1.6011976   0.96579826  1.        ]\n",
      " [ 1.          1.70075099  1.07085908  0.76114322  0.71548698  1.        ]\n",
      " [ 1.          0.74669634  0.77862063 -0.31782571  0.24267678  0.        ]\n",
      " [ 1.          0.60593418 -0.97481004  0.76114322 -1.28700325  1.        ]\n",
      " [ 1.         -0.86424839 -1.26704849  1.06942006 -0.78638069  1.        ]\n",
      " [ 1.          2.38892156 -0.6825716  -0.75712021 -0.49435087  1.        ]\n",
      " [ 1.         -1.36473607  1.55792315 -0.66463716 -0.52216323  0.        ]]\n",
      "\n",
      "First 10 testing examples:\n",
      "[[ 1.          0.48081226 -0.6825716   0.05210649  0.59033134  1.        ]\n",
      " [ 1.         -0.87988863 -0.29292034 -0.387188    0.0758026   0.        ]\n",
      " [ 1.          1.09078163  1.75274879 -0.80336173  0.72939316  0.        ]\n",
      " [ 1.         -0.2855595  -0.39033315 -1.35055312  0.08970878  0.        ]\n",
      " [ 1.         -0.73912647  1.07085908 -1.58946767  1.16048481  0.        ]\n",
      " [ 1.          0.8092573  -1.26704849  0.40662486 -0.99497343  1.        ]\n",
      " [ 1.          1.18462307  1.8501616  -0.84960326 -1.12012907  1.        ]\n",
      " [ 1.         -1.23961415 -0.77998441  0.54534943 -0.50825705  1.        ]\n",
      " [ 1.         -0.16043758 -0.87739723 -1.65882996  0.71548698  0.        ]\n",
      " [ 1.          0.48081226 -1.16963567 -0.70317176  1.20220335  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows of training/testing data:  (will be useful to have these later)\n",
    "\n",
    "print(\"First 10 training examples:\")\n",
    "print(np.hstack([X_train, y_train.reshape(-1, 1)])[0:10])\n",
    "print()\n",
    "print(\"First 10 testing examples:\")\n",
    "print(np.hstack([X_test, y_test.reshape(-1, 1)])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f575209-8473-461d-99d3-51e482549cc7",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Like in Part A of the previous project, we will rely on an external method to create\n",
    "a logistic regression model for us, then we will see if we can replicate it ourselves.\n",
    "\n",
    "Below is code that uses scikit-learn to do this for us.  Don't worry too much about what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "102fdfea-65d7-4803-9ea4-c2fa5f56e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 16.5 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------- 298.0/298.0 kB 18.0 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\muwah\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\muwah\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "w found through scikit-learn: [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n"
     ]
    }
   ],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, penalty=None, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "w_direct = model.coef_[0]\n",
    "\n",
    "print(\"w found through scikit-learn:\", w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e005-4294-4c93-a8a7-227c6db31309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, write a sentence about how to interpret these \n",
    "# numbers in w_direct, in particular, (1) why are some negative\n",
    "# and some positive, and (2) what is the special interpretation of\n",
    "# w_direct[0]?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "# The negative weights indicate that the corresponding input feature has\n",
    "# a negative influence on the predicted probability of the target feature so it\n",
    "# is negative to reduce the error between the predicted probabilities and the\n",
    "# true labels in the training data. \n",
    "# w_direct[0] is the bias variable that captures the baseline probability of\n",
    "# the target variable i.e what the probability would have been if the other inputs\n",
    "# were 0.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031723e-d89c-4613-8eb7-a8bd7cd16b64",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In this part you will write code for binary logistic regression by hand, including the model,\n",
    "the loss function, the cost function, and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79ea1eb0-3006-4789-a7da-50bd839697f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid function\n",
    "\n",
    "# Write code here to define the sigmoid function 1/(1+e^-x).  \n",
    "# IMPORTANT: Use the np.exp function to raise e to a power.  \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0d7adf2-20fe-4f18-a59f-863ffaf8f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.2689414213699951 0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(sigmoid(0), sigmoid(-1), sigmoid(0.5))\n",
    "\n",
    "# should print 0.5 0.2689414213699951 0.6224593312018546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb4f2961-7789-4ff7-8aec-35e140601a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called run_model below to run the logistic\n",
    "# regression model on one feature vector (x_data).\n",
    "# In other words, this function should compute 1/(1 + e^(-wx))\n",
    "# where x is x_data.  But do this by calling your sigmoid function\n",
    "# and the dot product function (np.dot()).\n",
    "\n",
    "def run_model(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    return sigmoid(np.dot(x_data, w))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d36a583-5503-4b8b-ace8-851d3b0e5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986297185506662"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: run the model from Part A on the first testing example\n",
    "\n",
    "run_model(w_direct, X_train[0])  # should be 0.9986297185506662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "111adf69-bbe2-4bab-a09e-161deb3aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# The run_model function only returns numbers in a certain range.  What is this range\n",
    "# and why does this function not return numbers outside of that range?\n",
    "\n",
    "# ANSWER:\n",
    "# This range is between 0 and 1. Mathematically, the model's parameters \n",
    "# only affect the denominator which can range from 1 to infinity so the model\n",
    "# can only return values in that range. Technically, this model returns the\n",
    "# probability of the target variable being in a certain class so it can only\n",
    "# fall between 0 and 1.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5638376f-586d-4fdc-bb96-4a1d4d322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called make_prediction that will\n",
    "# actually predict the class 0 or 1 for a feature vector x_data.\n",
    "# To do this, just call run_model and check if the return\n",
    "# value is > or < than 0.5\n",
    "\n",
    "def make_prediction(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_model(x_data, w) > 0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7e4e0e9-0efc-46c1-a012-2da1a7b05ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for test example 0 -> 1\n",
      "Predicted class for test example 1 -> 0\n",
      "Predicted class for test example 2 -> 0\n",
      "Predicted class for test example 3 -> 0\n",
      "Predicted class for test example 4 -> 0\n",
      "Predicted class for test example 5 -> 1\n",
      "Predicted class for test example 6 -> 1\n",
      "Predicted class for test example 7 -> 1\n",
      "Predicted class for test example 8 -> 0\n",
      "Predicted class for test example 9 -> 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: classify the first few testing examples using the model from Part A\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Predicted class for test example\", i, \"->\", make_prediction(X_test[i], w_direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3309fc50-bf06-4b31-9740-6f13f1b5eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Given the output immediately above, what is the accuracy of the model in Part A (since we used w_direct\n",
    "# above) just based on these 10 training examples?  (Answer as a percent; in other words\n",
    "# the percentage of those 10 testing examples that were predicted correctly).\n",
    "\n",
    "# ANSWER:\n",
    "# 100%\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56291ae2-12bf-498b-9500-df3f1d66a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called compute_accuracy that takes a \n",
    "# set of X values and a set of y values and a parameter vector\n",
    "# w.  This function should predict the class for each example x\n",
    "# in X_data and based on the true y values (y_data), compute\n",
    "# the accuracy on this data set.\n",
    "\n",
    "# To do this, call make_prediction on each row of X_data\n",
    "# and compare the output against the corresponding value in y_data.\n",
    "# Count how many predictions are correct and divide by the total.\n",
    "\n",
    "def compute_accuracy(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix of features (flexible rows, n+1 cols)\n",
    "    y_data: vector of true classes (same number of rows as X_data)\n",
    "    w: array of weights (n+1)\n",
    "    returns: percentage of rows in X_data classified correctly\n",
    "    \"\"\"\n",
    "    \n",
    "    totalCorrect = 0\n",
    "    for i in range(0, X_data.shape[0]):\n",
    "        if make_prediction(X_data[i], w) == y_data[i]:\n",
    "            totalCorrect += 1\n",
    "    \n",
    "    return totalCorrect / X_data.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbcece4e-4d1c-4627-ac5d-bf70f9b9b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "train_acc_partA = compute_accuracy(X_train, y_train, w_direct)\n",
    "test_acc_partA = compute_accuracy(X_test, y_test, w_direct)\n",
    "\n",
    "print(train_acc_partA)  # should be 0.9625\n",
    "print(test_acc_partA)  # should be 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff4b3493-862b-42f9-b0cc-5dfa761729fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Which of the two numbers above do we report as the \"true\" accuracy of our model,\n",
    "# and why do we typically not report the other (or not give it as much importance)?\n",
    "\n",
    "# ANSWER:\n",
    "# We report the second one because the first one is the training data that the\n",
    "# model has been trained on and the model may overfit and give a higher percentage\n",
    "# of correct predictions than it actually will when run on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f7840f3-d429-4302-baa9-5d90c1435740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_loss function below to compute the\n",
    "# loss over *one* training example, given the true y value\n",
    "# and the predicted y value (y_hat).  \n",
    "# This is the logistic regression loss function.\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    y: 0 or 1 \n",
    "    y_hat: decimal number between 0 and 1\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -1 * math.log(y_hat) \n",
    "    else:\n",
    "        return -1 * math.log(1 - y_hat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9d6a2e2-9c16-45fa-a359-0f83a9d3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w.\n",
    "# Call your run_model() and compute_loss() functions \n",
    "# that you defined above.  You should have one loop.\n",
    "# DO NOT CALL MAKE_PREDICTION; it's not needed here.\n",
    "\n",
    "def compute_cost(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    totalCost = 0\n",
    "    for i in range(0, X_data.shape[0]):\n",
    "        totalCost += compute_loss(y_data[i], run_model(X_data[i], w))\n",
    "    \n",
    "    return totalCost / X_data.shape[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40f04248-f046-4865-9246-9d27d63afd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07352472835636173\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compute the loss for w_direct from Part A:\n",
    "\n",
    "w_direct_cost = compute_cost(X_train, y_train, w_direct)  # This is the minimum cost we can ever get!  Should be less than 0.1.\n",
    "print(w_direct_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca88aeed-7b1a-40fc-a742-ba127c2a670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# compute_cost() above returns a single number, but based on the formula in J(w),\n",
    "# this function can only ever return numbers in a fixed range.  What is that range\n",
    "# and why are only numbers in this range ever returned?\n",
    "\n",
    "# ANSWER:\n",
    "# The cost function return an error within the range of 0 to infinity.\n",
    "# It returns 0 only if the predicted probabilities are correct and a higher\n",
    "# number as the predictions deviate more from the original target value.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed2c1a63-7376-41db-8235-cfee2ef8dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(w).  \n",
    "# Do not use matrix computations here; call your run_model() function\n",
    "# that you defined above.  You should have two nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    arr_grad = []\n",
    "    for j in range(0,X_data.shape[1]):\n",
    "        total_w = 0\n",
    "        for i in range(0, X_data.shape[0]):\n",
    "            y = y_data[i] # y(i)\n",
    "            y_hat = run_model(X_data[i], w)\n",
    "            total_w += (y_hat - y) * X_data[i][j]\n",
    "        arr_grad.append(total_w / X_data.shape[0])\n",
    "    return np.array(arr_grad)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7645ea34-7117-4971-abca-4b7084038f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 0.0735627676878601\n",
      "Final w: [ 2.10575094  6.2063935  -6.27581763  8.59091234 -9.34041149]\n"
     ]
    }
   ],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "w_manual = np.zeros(X_train.shape[1])  # n+1 weights\n",
    "w_manual_cost = 0\n",
    "J_list = []\n",
    "ALPHA = 30\n",
    "\n",
    "for i in range(0, 200):\n",
    "    w_manual_cost = compute_cost(X_train, y_train, w_manual)\n",
    "    J_list.append(w_manual_cost)\n",
    "    wj = compute_gradient(X_train, y_train, w_manual)\n",
    "    w_manual = w_manual - ALPHA * wj\n",
    "\n",
    "# Verify:\n",
    "print(\"Final cost:\", w_manual_cost)\n",
    "print(\"Final w:\", w_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "95163d93-73ab-4fed-8cc6-0a464f0c74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b65c2f-022c-41b8-b9ae-a47c3e847e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# My initial choice was 0.0001 and my final choice was 30. At first, the curve\n",
    "# was not flattening so I realized that alpha was small and gradient \n",
    "# descent was not able to converge in time.I kept increasing alpha as \n",
    "# the cost kept going down.\n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# 200\n",
    "#\n",
    "# What was your final vector of weights? (w_manual)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# [ 2.10575094  6.2063935  -6.27581763  8.59091234 -9.34041149]\n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# 0.0735627676878601\n",
    "#\n",
    "# What was your final vector of weights from Part A? (w_direct)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n",
    "#\n",
    "# What cost of these weights? (w_direct_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# 0.07352472835636173\n",
    "#\n",
    "# How close are your weights from Part B to the \"correct\" weights from Part A?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "# Very close. The biggest weight difference is of around 0.3\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b703235e-8ccc-4842-8b80-c28b1a8a38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partB and test_acc_partB.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_acc_partB = compute_accuracy(X_train, y_train, w_manual)\n",
    "test_acc_partB = compute_accuracy(X_test, y_test, w_manual)\n",
    "\n",
    "print(train_acc_partB)  \n",
    "print(test_acc_partB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c9d1b-8cc9-40b9-b588-4a9ca859b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# How does the accuracy of the model you created by hand in Part B compare to \n",
    "# the accuracy of the model from Part A created by scikit-learn?\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "# They are exactly the same\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bdf7cf7e-37dc-4fb5-a08b-876e1140c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "Weights: [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n",
      "Cost: 0.07352472835636173\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n",
      "Part B\n",
      "Weights: [ 2.10575094  6.2063935  -6.27581763  8.59091234 -9.34041149]\n",
      "Cost: 0.0735627676878601\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", w_direct)\n",
    "print(\"Cost:\", w_direct_cost)\n",
    "print(\"Training accuracy:\", train_acc_partA)\n",
    "print(\"Testing accuracy:\", test_acc_partA)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", w_manual)\n",
    "print(\"Cost:\", w_manual_cost)\n",
    "print(\"Training accuracy:\", train_acc_partB)\n",
    "print(\"Testing accuracy:\", test_acc_partB)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf88d4-1db9-48a1-bca7-44e87b9c2dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
